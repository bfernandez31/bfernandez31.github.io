---
command: '/verify'
category: 'Testing & Quality'
purpose: 'Systematic test failure resolution with intelligent analysis'
wave-enabled: false
performance-profile: 'complex'
---

# /verify - Test Failure Resolution Command

Systematically analyze and fix test failures with evidence-based approach.

## Command Modes

### Mode 1: Full Analysis (no arguments)

```bash
/verify
```

- Reads `test-failures.json` (generated by verify workflow)
- Runs ALL tests if no report exists
- Analyzes failures systematically using --think-hard + Sequential MCP
- Fixes issues iteratively with incremental validation
- Re-runs only affected tests after each fix

### Mode 2: Targeted Fix (with test paths)

```bash
/verify tests/api/tickets.spec.ts tests/e2e/board.spec.ts
```

- Analyzes specific test files only
- Faster iteration for known failures
- Useful for debugging specific test suites

## Execution Strategy

### Phase 1: Failure Analysis (--think-hard + Sequential MCP)

**CRITICAL CONTEXT**:

- ‚ÑπÔ∏è **ALL TESTS WERE PASSING ON MAIN BRANCH (100% baseline)**
- ‚ÑπÔ∏è **Test failures are expected when implementing new features**
- üí° **Your job**: Determine if failure is a bug OR intentional behavior change

**Two Types of Test Failures**:

1. **Implementation Bug** (needs fix):
   - Test expects correct behavior, implementation has a bug
   - Example: Test expects 5 tickets, code returns 3 due to accidental filter
   - Action: Fix the implementation bug

2. **Intentional Behavior Change** (needs test update):
   - New feature changes expected behavior
   - Test assertions need updating to reflect new behavior
   - Example: New feature adds pagination, test needs to expect paginated response
   - Action: Update test assertions to match new specification

**Read structured failure report**:

```typescript
interface FailureReport {
  summary: {
    totalFailures: number;
    unitFailures: number;
    e2eFailures: number;
  };
  categories: {
    assertions: TestFailure[]; // Wrong expectations
    timeouts: TestFailure[]; // Performance issues
    errors: TestFailure[]; // Runtime crashes
    setup: TestFailure[]; // Test environment issues
  };
  rootCauses: RootCause[]; // Grouped by common cause
  impactPriority: string[]; // Fix order by impact
}
```

**Use Sequential MCP for deep analysis**:

- Identify patterns across failures
- Group related failures by root cause
- Prioritize by impact (number of affected tests)
- **CRITICAL**: Determine if failure is bug or intentional behavior change
- **Decision Framework**:
  1. Read specification (`specs/*/spec.md`) for intended behavior
  2. Compare failing test with specification requirements
  3. If spec requires new behavior ‚Üí Update test to match spec
  4. If implementation doesn't match spec ‚Üí Fix implementation
  5. When unclear ‚Üí Default to fixing implementation (safer)

### Phase 2: Systematic Fixing

**For each root cause (highest impact first)**:

1. **Gather Context** (parallel tool calls):
   - Read test file content
   - Read implementation being tested
   - **CRITICAL**: Check what changed: `git diff main...HEAD`
   - **CRITICAL**: Check if test existed on main: `git show main:<test-file>`
   - Find related passing tests (for patterns)
   - Review error stack traces

2. **Analyze with Sequential MCP**:
   - **FIRST**: Read specification to understand intended behavior
   - Why is test failing now?
   - What changes on this branch caused the failure?
   - Is the failure expected (new behavior) or a bug?
   - Are there related failures with same root cause?
   - What's the minimum change to fix?

**Example Analysis Case 1: Implementation Bug**

```
Test: tests/api/tickets.spec.ts > "GET /tickets returns correct count"
Failure: Expected 5 but got 3

Step 1: Read specification
$ cat specs/042-ticket-api/spec.md
"GET /tickets should return ALL tickets for the project"

Step 2: What changed in implementation?
$ git diff main...HEAD -- app/api/projects/[projectId]/tickets/route.ts
+ const tickets = await prisma.ticket.findMany({
+   where: { projectId, stage: 'INBOX' }  // ‚Üê ACCIDENTAL FILTER!
+ })

Step 3: Root cause identified
- Specification says: Return ALL tickets
- Implementation has accidental filter: Returns only INBOX tickets
- This is a BUG - not intended behavior

Step 4: Decision
‚Üí Fix implementation: Remove the accidental stage filter
‚Üí Do NOT fix test - test expectations match specification
```

**Example Analysis Case 2: Intentional Behavior Change**

```
Test: tests/api/tickets.spec.ts > "GET /tickets returns correct response format"
Failure: Expected { tickets: [...] }, got { tickets: [...], pagination: {...} }

Step 1: Read specification
$ cat specs/042-pagination/spec.md
"Add pagination to ticket listing API. Response format:
{
  tickets: Ticket[],
  pagination: { page, pageSize, total }
}"

Step 2: Compare with test expectations
Test expects: { tickets: [...] }
Spec requires: { tickets: [...], pagination: {...} }

Step 3: Root cause identified
- Specification requires NEW response format with pagination
- Test still expects OLD response format (before pagination)
- This is INTENTIONAL BEHAVIOR CHANGE

Step 4: Decision
‚Üí Update test: Change assertion to expect new pagination format
‚Üí Do NOT fix implementation - it correctly implements the spec
```

3. **Apply Fix**:
   - Update implementation OR test (evidence-based decision)
   - Use Edit tool for precise changes
   - Commit with descriptive message:

     ```
     test: fix [category] in [test-file]

     Root cause: [explanation]
     Fixed by: [description of change]
     Affects: [list of test cases]
     ```

4. **Incremental Verification**:
   - Re-run ONLY affected tests (not full suite)
   - If pass: Mark root cause resolved, continue to next
   - If fail: Analyze deeper with --think-hard, try alternative fix
   - Maximum 3 attempts per root cause before escalation

5. **Quality Gate** (after each fix):
   - Run lint on [affected-files]
   - Run typecheck
   - Ensure no new failures introduced

### Phase 3: Final Validation

**Run full test suite**:

**Outcomes**:

- ‚úÖ **All Pass**: Commit all fixes, exit successfully (workflow creates PR)
- ‚ùå **Some Fail**: Report remaining failures with analysis, exit with error (workflow stops)

## Best Practices

### 1. Evidence-Based Decisions

**When to fix test vs. implementation**:

**Analysis Framework** (use in this order):

1. **Read Specification FIRST** (`specs/*/spec.md`)
   - Understand the intended behavior for this feature
   - Check if new behavior is documented in spec
   - Specification is source of truth

2. **Compare Test Expectations with Spec**:
   - Does test expect old behavior (before this feature)?
   - Does spec document new behavior that breaks old tests?
   - Is the test assertion aligned with specification?

3. **Make Evidence-Based Decision**:

   **Case A: Fix Implementation** (common for bugs):
   - Specification says X, implementation does Y
   - Test expects X (correct), implementation returns Y (wrong)
   - Action: Fix implementation to match specification

   **Case B: Update Test** (common for behavior changes):
   - Specification says NEW behavior X
   - Test expects OLD behavior Y (before this feature)
   - Implementation correctly does X (matches spec)
   - Action: Update test to expect X (new behavior from spec)

   **Case C: Fix Both** (rare):
   - Specification unclear or contradictory
   - Implementation partially wrong
   - Test has some incorrect assertions
   - Action: Fix implementation AND update test

**Analysis Workflow**:

```bash
# Step 1: Read specification
cat specs/{num}-{description}/spec.md

# Step 2: Understand what changed
git diff main...HEAD

# Step 3: Compare test file versions
git diff main...HEAD -- tests/api/tickets.spec.ts

# Step 4: Decide based on spec
# - If implementation violates spec ‚Üí Fix implementation
# - If test expects old behavior, spec documents new ‚Üí Update test
```

**Evidence sources (priority order)**:

1. **Specification** (`specs/*/spec.md`) - Source of truth for intended behavior
2. **Git diff** (`git diff main...HEAD`) - What changed in this branch
3. **Test history** (`git show main:<test-file>`) - What was passing before
4. Related passing tests (patterns and conventions)
5. API documentation (for external libraries)

### 2. Failure Categorization

**Assertion Failures** (test expects A, got B):

- Check if expectation matches spec
- Verify implementation logic
- Look for off-by-one errors, edge cases

**Timeout Failures** (test exceeds time limit):

- Check for infinite loops
- Review async operation handling
- Look for missing `await` keywords
- Increase timeout if legitimate slow operation

**Runtime Errors** (crashes, exceptions):

- Check null/undefined handling
- Verify type safety
- Review error boundaries
- Check for missing dependencies

**Setup Failures** (test environment issues):

- Verify database state
- Check test isolation
- Review global setup/teardown
- Validate test data fixtures

### 3. Incremental Testing Strategy

**After each fix**:

```bash
# Run only affected tests (fast feedback)

# If pass, run related test suite

# Only run full suite after all fixes applied
```

### 4. Git Workflow

**Commit strategy**:

- One commit per root cause fix (logical grouping)
- Descriptive messages with context
- Push after each successful fix (incremental progress)

**Example commits**:

```
test: fix ticket transition assertions

Root cause: Expected stage was outdated after schema change
Fixed by: Updated assertions to match new Stage enum values
Affects: tests/api/ticket-transition.spec.ts (3 test cases)
```

## Error Recovery

### If stuck on a failure

1. Use `--think-hard` for deeper analysis
2. Search codebase for similar patterns: `Grep "similar-test-pattern"`
3. Check git history: `git log -p --grep="related-feature"`
4. Read related documentation: Use Context7 MCP for library docs
5. Report to user with evidence if unfixable

### If tests are still failing after fixes

1. Summarize remaining failures with categories
2. Provide analysis of why fixes didn't work
3. Suggest next steps (e.g., manual intervention needed)
4. Exit with error code 1 (workflow will stop, no PR created)

## Performance Optimization

### Resource Management

- Run tests incrementally (not full suite every time)
- Use `--uc` flag if token usage >75%
- Batch related fixes when possible
- Use Task tool for parallel test analysis if >10 failures

### Time Budget

- Aim to complete within 15-20 minutes
- If >20 minutes, report progress and remaining work
- Prioritize high-impact fixes (multiple tests affected)

## Quality Standards

**Completion Criteria**:

- ‚úÖ All tests pass (100% success rate)
- ‚úÖ No lint errors introduced
- ‚úÖ No type errors introduced
- ‚úÖ All fixes committed with descriptive messages
- ‚úÖ Final validation run successful

**DO NOT**:

- ‚ùå Skip failing tests without fixing root cause
- ‚ùå Disable tests to make suite pass
- ‚ùå Make superficial fixes without understanding root cause
- ‚ùå Increase timeouts excessively without justification
- ‚ùå Mark complete if any tests still failing

## MCP Server Integration

**Sequential** (Primary): Deep failure analysis, root cause identification
**Context7** (Secondary): Library documentation, testing patterns
**Playwright** (Tertiary): E2E test debugging, browser automation issues

## Example Workflow

```
User: /verify
```

